<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on S. M. Hasan</title>
    <link>https://numan947.github.io/tags/nlp/</link>
    <description>Recent content in NLP on S. M. Hasan</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Sep 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://numan947.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Demystifying RNNs: A Deep Dive into Dimensions and Parameters</title>
      <link>https://numan947.github.io/posts/rnn-1/</link>
      <pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://numan947.github.io/posts/rnn-1/</guid>
      <description>&lt;h1 id=&#34;demystifying-rnns-a-deep-dive-into-dimensions-and-parameters&#34;&gt;Demystifying RNNs: A Deep Dive into Dimensions and Parameters&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;Understanding what really happens inside Recurrent Neural Networks&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;When learning about Recurrent Neural Networks (RNNs), many tutorials focus on the high-level concept of &amp;ldquo;memory&amp;rdquo; but gloss over the practical details of how they actually work. As someone who struggled with these details, I want to share the insights that finally made RNNs click for me.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-core-rnn-equations&#34;&gt;The Core RNN Equations&lt;/h2&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start with the fundamental RNN equations that everyone shows:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Efficient Multilingual Feature Extraction for Edge NLP</title>
      <link>https://numan947.github.io/projects/effe/</link>
      <pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://numan947.github.io/projects/effe/</guid>
      <description>&lt;h1 id=&#34;llm-powered-feature-extraction-for-edge&#34;&gt;LLM-Powered Feature Extraction for Edge&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;Paper under review&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://numan947.github.io/projects/effe/eval_effe.png&#34; alt=&#34;Pareto Frontier&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;This research introduces a novel approach for deploying multilingual NLP on edge devices by rethinking the role of large language models. Instead of running full LLM inference, the framework leverages them as static feature repositories, bypassing the computationally expensive transformer stack.&lt;/p&gt;&#xA;&lt;p&gt;The method enables a tunable efficiency hierarchy, from ultra-efficient static embeddings to enhanced hybrid features via lightweight distillation. By using multilingual LLMs as a unified source, it eliminates the complexity of managing language-specific pipelines, providing a scalable path to on-device intelligence without the traditional overhead.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>

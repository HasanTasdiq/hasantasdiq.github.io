


















<!DOCTYPE html>
<html lang='en-us'><head>
    <meta charset="utf-8">
    <link rel="shortcut icon" href='https://numan947.github.io/favicon.ico' type="image/x-icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RL Notes: Huggin Face RL Course - S. M. Hasan</title>

    

    

    
    <meta name="author" content="Your Name" />
    

    
        <meta property="og:url" content="https://numan947.github.io/posts/hfrl-1/">
  <meta property="og:site_name" content="S. M. Hasan">
  <meta property="og:title" content="RL Notes: Huggin Face RL Course">
  <meta property="og:description" content="HFRL Unit-1 Summary Reinforcement Learning is a method where an agent learns by interacting with its environment, using trial and error and feedback from rewards.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-10T00:00:00+00:00">
    <meta property="article:tag" content="RL">
    <meta property="article:tag" content="Neural Network">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Hugging Face">

    

    
        
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RL Notes: Huggin Face RL Course">
  <meta name="twitter:description" content="HFRL Unit-1 Summary Reinforcement Learning is a method where an agent learns by interacting with its environment, using trial and error and feedback from rewards.">

    <link rel="stylesheet" href="/style.min.75a70fdd4c8502cf61b8e7faea82276e016de6c2be066b92fa16bdae1c1f873d.css" integrity="sha256-dacP3UyFAs9huOf66oInbgFt5sK&#43;BmuS&#43;ha9rhwfhz0=">





    
    <script>
        if (!('theme' in localStorage)) {
            localStorage.theme = 'light';
        }

        if (localStorage.theme === 'dark' || (!('theme' in localStorage) && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
            document.documentElement.setAttribute("data-theme", "dark");
        } else {
            document.documentElement.setAttribute("data-theme", "light");
        }
    </script>
<script defer src="/js/theme-switch.7c57075675400a1a12bc3fcfc744dd74e1e417b8db11fdef378d7a7ef1cc9e3f.js" integrity="sha256-fFcHVnVAChoSvD/Px0TddOHkF7jbEf3vN416fvHMnj8="></script>



    <script defer src="/js/hide-navbar-on-scroll.b5f3414715b82a9bc2c9086fc860ad5d0a63f67251e4dd4fa61e9dfce91ebdb6.js" integrity="sha256-tfNBRxW4KpvCyQhvyGCtXQpj9nJR5N1Pph6d/OkevbY="></script>





    <script defer src="/js/zooming.10d718ecdb4a98eab370ed60963ea87f1c612ac225609eadc52f0e536bc1517c.js" integrity="sha256-ENcY7NtKmOqzcO1glj6ofxxhKsIlYJ6txS8OU2vBUXw="></script>




    <script src="/js/math.5a8dbbda075325162f7a690e0de21e861a1794248ace5298c0d8479f0ed55ac8.js" integrity="sha256-Wo272gdTJRYvemkODeIehhoXlCSKzlKYwNhHnw7VWsg="></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.3/tex-mml-chtml.js"></script>











    
</head>
<body><header>
    <div id="header_content">
        <div id="header_left">
            <div id="sidebar_btn">
                <input type="checkbox" id="sidebar_btn_input" class="hidden" />
                <label id="sidebar_btn_label" for="sidebar_btn_input">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1.5rem" height="1.5rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
</svg>

</label>
                <label id="sidebar_canvas_overlay_wrapper" for="sidebar_btn_input">
                    <div id="sidebar_canvas_overlay"></div>
                </label>
                <div id="sidebar">
                    <ul><li>
                                <a href="/about/">About Me</a></li><li>
                                <a href="/research/">Research</a></li><li>
                                <a href="/publications/">Publications</a></li><li>
                                <a href="/posts/">Bits, Bytes &amp; Life</a></li></ul>
                </div>
            </div>
        
            <div class="brand">
                <div>
                    <a href="/">S. M. Hasan</a>
                </div>
            </div><nav id="header_navbar" class="pure-menu header-menu">
    <ul class="pure-menu-list"><li class="header-menu-item pure-menu-item ">
                    
                        <a href="/about/" class="pure-menu-link">About Me</a>
                    
                </li><li class="header-menu-item pure-menu-item ">
                    
                        <a href="/research/" class="pure-menu-link">Research</a>
                    
                </li><li class="header-menu-item pure-menu-item ">
                    
                        <a href="/publications/" class="pure-menu-link">Publications</a>
                    
                </li><li class="header-menu-item pure-menu-item insection">
                    
                        <a href="/posts/" class="pure-menu-link">Bits, Bytes &amp; Life</a>
                    
                </li></ul>
</nav>
</div>

        <div id="header_right">
            

            <div id="theme_tool">
                <button id="dark_mode_btn" class="header-menu-btn outline-button" title='Switch to dark mode' type="button">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1.5rem" height="1.5rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>

</button>
                <button id="light_mode_btn" class="header-menu-btn outline-button" title='Switch to light mode' type="button">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1.5rem" height="1.5rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
</svg>

</button>
            </div>

            
        </div>
    </div>
</header><div id="search_menu_wrapper" class="hidden">
    <div id="search_menu">
        <div id="search_menu_toolbar">
            <div id="search_menu_input_wrapper">
                <input id="search_menu_input" type="text" placeholder='Search Posts'>
            </div>
            <div id="search_menu_close_btn">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1.25rem" height="1.25rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="18" y1="6" x2="6" y2="18"></line><line x1="6" y1="6" x2="18" y2="18"></line>
</svg>

</div>
        </div>
        <div id="search_menu_results">
        </div>
    </div>
</div>
<main>
            <div id="content" class="content-margin">
                
    
    
        
        <div class="collapsible-menu-wrapper"><div class="collapsible-menu-type"><span>Table of contents</span></div><div class="collapsible-menu">
        
            <nav id="TableOfContents">
  <ul>
    <li><a href="#hfrl-unit-1">HFRL Unit-1</a>
      <ul>
        <li><a href="#summary">Summary</a></li>
        <li><a href="#two-ways-to-find-optimal-policy">Two ways to find Optimal Policy</a></li>
      </ul>
    </li>
    <li><a href="#hfrl-unit-2">HFRL Unit-2</a>
      <ul>
        <li><a href="#value-based-methods">Value-based methods</a></li>
        <li><a href="#bellman-equations">Bellman Equations</a></li>
        <li><a href="#monte-carlo-methods">Monte Carlo Methods</a></li>
        <li><a href="#temporal-difference-td-learning">Temporal Difference (TD) Learning</a></li>
        <li><a href="#on-policy-vs-off-policy-learning">On-policy vs Off-policy Learning</a></li>
        <li><a href="#q-learning">Q-learning</a></li>
        <li><a href="#q-learning-algorithm">Q-learning Algorithm</a></li>
        <li><a href="#off-policy-vs-on-policy">Off-policy vs On-policy</a></li>
      </ul>
    </li>
    <li><a href="#hfrl-unit-3">HFRL Unit-3</a>
      <ul>
        <li><a href="#q-function-recap">Q-Function Recap</a></li>
        <li><a href="#deep-q-networks-dqn">Deep Q-Networks (DQN)</a></li>
        <li><a href="#preprocessing-the-input-and-temporal-limits">Preprocessing the Input and Temporal Limits</a></li>
        <li><a href="#the-deep-q-learning-algorithm">The Deep Q-Learning Algorithm</a></li>
      </ul>
    </li>
  </ul>
</nav>
        
    </div></div>
    



    <div class="content-margin">



<article class="line-numbers">
    
    

    
    
        
        
        
    
        
        
            
            
            
        
        <section id="hfrl-unit-1">


<h2 class="header-anchor-wrapper"><a href="https://huggingface.co/learn/deep-rl-course/en/unit1">HFRL Unit-1</a>
  <a href="#hfrl-unit-1" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h2>

</section>
    
        
        
            
            
            
        
        <section id="summary">


<h3 class="header-anchor-wrapper">Summary
  <a href="#summary" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>
<p>Reinforcement Learning is a method where an agent learns by interacting with its environment, using trial and error and feedback from rewards.</p>
</li>
<li>
<p>The goal of an RL agent is to maximize its expected cumulative reward, based on the idea that all goals can be framed as maximizing this reward.</p>
</li>
<li>
<p>The RL process is a loop that outputs a sequence of state, action, reward, and next state.</p>
</li>
<li>
<p>Expected cumulative reward is calculated by discounting future rewards, giving more weight to immediate rewards since they are more predictable than long-term ones.</p>
</li>
<li>
<p>To solve an RL problem, we find an optimal policy, the AI&rsquo;s &ldquo;brain&rdquo; that decides the best action for each state to maximize expected return.</p>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="two-ways-to-find-optimal-policy">


<h3 class="header-anchor-wrapper">Two ways to find Optimal Policy
  <a href="#two-ways-to-find-optimal-policy" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ol>
<li><strong>Policy-based</strong> methods directly optimize the policy by adjusting its parameters to maximize expected return.</li>
<li><strong>Value-based</strong> methods train a value function that estimates the expected return for each state and use it to define the policy.</li>
</ol>
<p>Deep RL is &ldquo;Deep&rdquo; because it uses deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based).</p>
</section>
    
        
        
            
            
            
        
        <section id="hfrl-unit-2">


<h2 class="header-anchor-wrapper"><a href="https://huggingface.co/learn/deep-rl-course/en/unit2">HFRL Unit-2</a>
  <a href="#hfrl-unit-2" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h2>

</section>
    
        
        
            
            
            
        
        <section id="value-based-methods">


<h3 class="header-anchor-wrapper">Value-based methods
  <a href="#value-based-methods" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>
<p>Value-based methods estimate the value of each state (or state-action pair) and derive the policy from these values.</p>
</li>
<li>
<p>We learn a value function that maps a state to the expected value of being at that state.
<img src="/blogs/tutorials/huggingfaceRL/vbm-1.jpg" alt="Value Function"></p>
</li>
<li>
<p>The value of a state is the expected discounted return obtained by following the policy from that state.</p>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="what-does-it-mean-to-follow-a-policy">


<h4 class="header-anchor-wrapper">What does it mean to &ldquo;follow a policy&rdquo;?
  <a href="#what-does-it-mean-to-follow-a-policy" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ul>
<li>The goal of an RL agent is to have an optimal policy $\pi^*$ that maximizes the expected return from any state.
<ul>
<li>Policy-based methods $\rightarrow$ directly train the policy to select what action to take given a state $\rightarrow \pi(a|s) \rightarrow$ We do not need to learn a value function.
<ul>
<li>Policy takes the current state as input and outputs an action or a distribution over possible actions.</li>
<li>We don&rsquo;t define the policy explicitly; instead, we learn it through interactions with the environment.
<img src="/blogs/tutorials/huggingfaceRL/two-approaches-2.jpg" alt="Policy-based"></li>
</ul>
</li>
<li>Value-based methods $\rightarrow$ trains the policy indirectly by learning a value function $\rightarrow V(s) \rightarrow$ We do not need to learn the policy explicitly.
<ul>
<li>The value function takes the current state as input and outputs the expected return (value) of that state.</li>
<li>The policy is not trained or learned directly $\rightarrow$ it is derived from the value function given specific rules.</li>
<li>For example, a common rule is to select the action that leads to the state with the highest value (greedy policy).
<img src="/blogs/tutorials/huggingfaceRL/two-approaches-3.jpg" alt="Value-based"></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="the-difference-between-value-based-and-policy-based-methods">


<h4 class="header-anchor-wrapper">The Difference between Value-based and Policy-based methods
  <a href="#the-difference-between-value-based-and-policy-based-methods" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ul>
<li>In policy-based methods, the policy is learned directly by training a neural network to output actions based on states.
<ul>
<li>Optimal policy $\pi^*$ is learned directly.</li>
</ul>
</li>
<li>In value-based methods, the policy is derived from a learned value function, which estimates the expected return of states.
<ul>
<li>Optimal policy $\pi^<em>$ is derived from the optimal value function $V^</em>(s)$ or the action-value function $Q^*(s, a)$.
<img src="/blogs/tutorials/huggingfaceRL/link-value-policy.jpg" alt="Link"></li>
</ul>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="two-types-of-value-functions">


<h4 class="header-anchor-wrapper">Two types of Value Functions
  <a href="#two-types-of-value-functions" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ol>
<li><strong>State-Value Function</strong> $V_\pi(s)$: Estimates the expected return starting from state $s$ and following policy $\pi$.
<ul>
<li>It gives the value of being in a particular state.</li>
<li>Formula:
$$V_\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]$$</li>
<li>Where $G_t$ is the return (cumulative discounted reward) from time step $t$.
<img src="/blogs/tutorials/huggingfaceRL/state-value-function-1.jpg" alt="State-Value Function"></li>
</ul>
</li>
<li><strong>Action-Value Function</strong> $Q_\pi(s, a)$: Estimates the expected return starting from state $s$, taking action $a$, and following policy $\pi$ thereafter.
<ul>
<li>It gives the value of taking a particular action in a particular state.</li>
<li>Formula:
$$Q_\pi(s, a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$$
<img src="/blogs/tutorials/huggingfaceRL/action-state-value-function-1.jpg" alt="Action-Value Function"></li>
</ul>
</li>
</ol>
<p><strong>Difference</strong>: The state-value function evaluates the value of being in a state, while the action-value function evaluates the value of taking a specific action in a state.</p>
<p><strong>Both</strong> case are used to derive the optimal policy $\pi^*$ by selecting actions that maximize the expected return.</p>
<p><strong>Problem</strong>: To calculate the value of a state or state-action pair, we need to know the expected return $G_t$, which depends on future rewards and the policy $\pi$. This can be computationally expensive and complex, especially in environments with many states and actions. Bellman equations provide a recursive way to compute these values efficiently.</p>
</section>
    
        
        
            
            
            
        
        <section id="bellman-equations">


<h3 class="header-anchor-wrapper">Bellman Equations
  <a href="#bellman-equations" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>
<p>Simplifies the computation of value functions ($V_\pi(s)$ and $Q_\pi(s, a)$) by breaking them down into immediate rewards and the value of subsequent states.</p>
</li>
<li>
<p>Instead of calculating the expected return for each state or each state-action pair, we can use the Bellman equation.</p>
</li>
<li>
<p>The Bellman equation expresses a stateâ€™s value recursively as the immediate reward plus the discounted value of the next state:
</p>
$$V(S_t) = R_{t+1} + \gamma V(S_{t+1})$$<p>
<img src="/blogs/tutorials/huggingfaceRL/bellman4.jpg" alt="Bellman Equation"></p>
</li>
<li>
<p>In summary, the Bellman equation simplifies value estimation by expressing it as the immediate reward plus the discounted value of the next state.</p>
</li>
<li>
<p>Monte Carlo methods and Temporal Difference (TD) learning are two primary approaches to estimate value functions in reinforcement learning.</p>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="monte-carlo-methods">


<h3 class="header-anchor-wrapper">Monte Carlo Methods
  <a href="#monte-carlo-methods" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>Monte Carlo waits until the end of an episode to calculate the total return ($G_t$) and then updates the value function ($V_\pi(s)$ or $Q_\pi(s, a)$) based on this return.</li>
<li>It requires complete episodes.
<img src="/blogs/tutorials/huggingfaceRL/monte-carlo-approach.jpg" alt="Monte Carlo Methods"></li>
<li>The update rule for the state-value function is:
$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$$</li>
<li>Where $\alpha$ is the learning rate, and $G_t$ is the total return from time step $t$.</li>
<li>Then restart the episode and repeat.
<img src="/blogs/tutorials/huggingfaceRL/MC-3p.jpg" alt="Monte Carlo Update"></li>
</ul>
<p><strong>Example of Monte Carlo:</strong></p>
<ul>
<li>We initialize the value function $V(s)$ arbitrarily (e.g., all zeros) for all states $s$.</li>
<li>Our learning rate $\alpha$ is set to 0.1 and discount factor $\gamma$ is 1 (for simplicity).</li>
<li>We run an episode in the environment, collecting states, actions, and rewards until the episode ends.</li>
<li>At the end of the episode, we calculate the return $G_t$ for each time step $t$.</li>
<li>We update the value function for each state visited during the episode using the update rule.</li>
<li>We repeat this process for many episodes, gradually improving our value function estimates.</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="temporal-difference-td-learning">


<h3 class="header-anchor-wrapper">Temporal Difference (TD) Learning
  <a href="#temporal-difference-td-learning" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>TD learning updates the value function ($V_\pi(s)$ or $Q_\pi(s, a)$) at each time step using the immediate reward and the estimated value of the next state.</li>
<li>It does not require complete episodes and can learn from incomplete sequences.</li>
<li>Because we didn&rsquo;t wait until the end of the episode, we don&rsquo;t have the full return $G_t$.
<ul>
<li>Instead, we use the immediate reward $R_{t+1}$ and the estimated value of the next state $V(S_{t+1})$ to update our value function.</li>
<li>This is called bootstrapping because we are using our current estimate to improve itself incrementally.
<img src="/blogs/tutorials/huggingfaceRL/TD-1.jpg" alt="TD Learning"></li>
</ul>
</li>
<li>The update rule for the state-value function is:
$$V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$</li>
<li>Where $\alpha$ is the learning rate, $R_{t+1}$ is the immediate reward, and $V(S_{t+1})$ is the estimated value of the next state.</li>
<li>Then move to the next state and repeat.</li>
<li>This method is known as <strong>TD(0)</strong> because it updates the value function based on a one-step lookahead.</li>
<li>The estimated return is known as the TD target:
$$\text{TD Target} = R_{t+1} + \gamma V(S_{t+1})$$</li>
</ul>
<p><img src="/blogs/tutorials/huggingfaceRL/TD-1p.jpg" alt="TD(0) Update"></p>
</section>
    
        
        
            
            
            
        
        <section id="summary-of-monte-carlo-vs-td-learning">


<h4 class="header-anchor-wrapper">Summary of Monte Carlo vs TD Learning
  <a href="#summary-of-monte-carlo-vs-td-learning" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<table class="mc-table">
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Monte Carlo Methods</th>
          <th>Temporal Difference (TD) Learning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Update Timing</td>
          <td>Updates value function at the end of an episode</td>
          <td>Updates value function at each time step</td>
      </tr>
      <tr>
          <td>Requirement</td>
          <td>Requires complete episodes</td>
          <td>Can learn from incomplete sequences</td>
      </tr>
      <tr>
          <td>Return Calculation</td>
          <td>Uses total return $G_t$</td>
          <td>Uses immediate reward and estimated next state value</td>
      </tr>
      <tr>
          <td>Bootstrapping</td>
          <td>No</td>
          <td>Yes</td>
      </tr>
  </tbody>
</table>
</section>
    
        
        
            
            
            
        
        <section id="on-policy-vs-off-policy-learning">


<h3 class="header-anchor-wrapper">On-policy vs Off-policy Learning
  <a href="#on-policy-vs-off-policy-learning" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li><strong>On-policy</strong> methods learn the value of the policy being executed (the same policy used to make decisions).
<ul>
<li>Example: SARSA (State-Action-Reward-State-Action)</li>
</ul>
</li>
<li><strong>Off-policy</strong> methods learn the value of a different policy than the one being executed (the behavior policy).
<ul>
<li>Example: Q-learning</li>
</ul>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="q-learning">


<h3 class="header-anchor-wrapper">Q-learning
  <a href="#q-learning" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>
<p>Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function $Q(s, a)$.</p>
</li>
<li>
<p>Approximate the optimal action-value function $Q^*(s, a)$, which gives the maximum expected return for taking action $a$ in state $s$ and following the optimal policy thereafter.
<img src="/blogs/tutorials/huggingfaceRL/Q-function.jpg" alt="Q-Learning"></p>
</li>
<li>
<p>The value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy.</p>
</li>
<li>
<p>The reward is the feedback the agent gets from the environment after taking an action.</p>
</li>
<li>
<p>Q-function is encoded in a table called Q-table:</p>
<table class="mc-table">
  <thead>
      <tr>
          <th>State</th>
          <th>Action 1</th>
          <th>Action 2</th>
          <th>Action 3</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>S1</td>
          <td>Q(S1,A1)</td>
          <td>Q(S1,A2)</td>
          <td>Q(S1,A3)</td>
      </tr>
      <tr>
          <td>S2</td>
          <td>Q(S2,A1)</td>
          <td>Q(S2,A2)</td>
          <td>Q(S2,A3)</td>
      </tr>
      <tr>
          <td>S3</td>
          <td>Q(S3,A1)</td>
          <td>Q(S3,A2)</td>
          <td>Q(S3,A3)</td>
      </tr>
  </tbody>
</table>
</li>
<li>
<p>The agent selects actions based on the Q-values in the table, typically choosing the action with the highest Q-value for the current state (greedy policy).</p>
</li>
<li>
<p>The Q-values are updated using the Q-learning update rule:
</p>
$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$</li>
<li>
<p>Where $\alpha$ is the learning rate, $R_{t+1}$ is the immediate reward, $\gamma$ is the discount factor, and $\max_a Q(S_{t+1}, a)$ is the maximum estimated Q-value for the next state $S_{t+1}$ over all possible actions $a$.</p>
</li>
<li>
<p>The term $R_{t+1} + \gamma \max_a Q(S_{t+1}, a)$ is known as the TD target in Q-learning.</p>
</li>
<li>
<p>The agent updates the Q-value for the current state-action pair based on the immediate reward and the maximum estimated Q-value for the next state.</p>
</li>
<li>
<p>This update is done at each time step, allowing the agent to learn from each interaction with the environment.</p>
</li>
<li>
<p>Q-function returns the expected cumulative reward for taking action $a$ in state $s$ and following the optimal policy thereafter.</p>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="summary-of-q-learning">


<h4 class="header-anchor-wrapper">Summary of Q-learning
  <a href="#summary-of-q-learning" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ul>
<li>Trains Q-function $Q(s, a)$ $\rightarrow$ action-value function $\rightarrow$ internally uses Q-table that maps state-action pairs to values.</li>
<li>Given a state and an action, Q-function returns the expected cumulative reward for taking that action in that state and following the optimal policy thereafter.</li>
<li>After training, the optimal policy $\pi^*$ can be derived by selecting the action with the highest Q-value for each state:
$$\pi^*(s) = \arg\max_a Q^*(s, a)$$</li>
<li>Q-learning is an off-policy method because it learns the optimal policy independently of the</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="q-learning-algorithm">


<h3 class="header-anchor-wrapper">Q-learning Algorithm
  <a href="#q-learning-algorithm" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<p><img src="/blogs/tutorials/huggingfaceRL/Q-learning-2.jpg" alt="Q-learning Algorithm"></p>
<ul>
<li>Step 1: Initialize the Q-table with arbitrary values (e.g., all zeros) for all state-action pairs.</li>
<li>Step 2: Choose an action using the epsilon-greedy strategy
<img src="/blogs/tutorials/huggingfaceRL/Q-learning-4.jpg" alt="Epsilon-Greedy Strategy">
<ul>
<li>With probability $\epsilon$, select a random action (exploration).</li>
<li>With probability $1 - \epsilon$, select the action with the highest Q-value for the current state (exploitation).</li>
<li>As training progresses, $\epsilon$ is typically decayed to reduce exploration and increase exploitation.
<img src="/blogs/tutorials/huggingfaceRL/Q-learning-5.jpg" alt="Epsilon Decay"></li>
</ul>
</li>
<li>Step 3: Take the action, observe the reward and the next state, $A_t, R_{t+1}, S_{t+1}$.</li>
<li>Step 4: Update the Q-value for the current state-action pair using the Q-learning update rule.
<ul>
<li>Use the update formula to adjust the Q-value based on the observed reward and the maximum estimated Q-value for the next state.</li>
<li>To produce the TD target, we use the immediate reward $R_{t+1}$ and the maximum estimated discounted Q-value for the next state $S_{t+1}$ over all possible actions. This is called bootstrapping because we are using our current estimate to improve itself incrementally.
<img src="/blogs/tutorials/huggingfaceRL/Q-learning-7.jpg" alt="Bootstrapping">
<img src="/blogs/tutorials/huggingfaceRL/Q-learning-8.jpg" alt="Bootstrapping-2"></li>
<li>To get the TD target, we use:
<ul>
<li>We obtain the reward $R_{t+1}$ from the environment after taking action $A_t$ in state $S_t$.</li>
<li>To get the best state-action pair value for the next state $S_{t+1}$, we look at all possible actions $a$ in that state and select the one with the highest Q-value: $\max_a Q(S_{t+1}, a)$ $\rightarrow$ Note that this is not $\epsilon$-greedy; this is always taking the maximum value.</li>
<li>We multiply this maximum Q-value by the discount factor $\gamma$ to account for the time value of future rewards.</li>
<li>Finally, we add the immediate reward $R_{t+1}$ to the discounted maximum Q-value to get the TD target:
$$\text{TD Target} = R_{t+1} + \gamma \max_a Q(S_{t+1}, a)$$</li>
<li>Then when the update of this Q-value is done, we start in a new state and select another action using the $\epsilon$-greedy strategy.</li>
<li>This is why Q-learning is considered an <strong>off-policy method</strong> because the action used to update the Q-value (the one that maximizes the Q-value for the next state) is not necessarily the action that was actually taken (which could have been a random action due to exploration).</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="off-policy-vs-on-policy">


<h3 class="header-anchor-wrapper">Off-policy vs On-policy
  <a href="#off-policy-vs-on-policy" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>Off-policy: using a different policy for acting (inference) and learning (training).
<ul>
<li>Example: Q-learning $\rightarrow$ uses $\epsilon$-greedy for acting and greedy (max) for learning.</li>
</ul>
</li>
<li>On-policy: using the same policy for acting and learning.
<ul>
<li>Example: SARSA $\rightarrow$ uses $\epsilon$-greedy for both acting and learning.</li>
</ul>
</li>
</ul>
<p><img src="/blogs/tutorials/huggingfaceRL/off-on-4.jpg" alt="Off-policy vs On-policy"></p>
</section>
    
        
        
            
            
            
        
        <section id="hfrl-unit-3">


<h2 class="header-anchor-wrapper"><a href="https://huggingface.co/learn/deep-rl-course/en/unit3">HFRL Unit-3</a>
  <a href="#hfrl-unit-3" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h2>

<ul>
<li>Producing and updating the Q-table becomes challenging in environments with large or continuous state and action spaces.</li>
<li>Deep Q-Networks (DQN) address this by using neural networks to approximate the Q-function instead of a table.</li>
<li>DQN uses a neural network to take the state as input and output Q-values for all possible actions.</li>
</ul>
<p><img src="/blogs/tutorials/huggingfaceRL/deep.jpg" alt="DQN-basic"></p>
<p><img src="/blogs/tutorials/huggingfaceRL/deep-q-network.jpg" alt="DQN-architecture"></p>
</section>
    
        
        
            
            
            
        
        <section id="q-function-recap">


<h3 class="header-anchor-wrapper">Q-Function Recap
  <a href="#q-function-recap" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>The Q-function $Q(s, a)$ is an action-value function that determines the value of being at a particular state and taking a specific action at that state.</li>
<li>Q-Learning is the algorithm that trains the Q-function.</li>
<li>&ldquo;Q&rdquo; stands for &ldquo;quality,&rdquo; representing the quality of a state-action pair in terms of expected cumulative reward.</li>
<li>Internally Q-function is represented as a Q-table, which maps state-action pairs to their corresponding Q-values.</li>
<li>Issue: In environments with large or continuous state and action spaces, maintaining and updating a Q-table becomes impractical $\rightarrow$ Qtable doesn&rsquo;t scale well.</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="deep-q-networks-dqn">


<h3 class="header-anchor-wrapper">Deep Q-Networks (DQN)
  <a href="#deep-q-networks-dqn" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>DQN uses a neural network to approximate the Q-function, allowing it to handle large or continuous state spaces.</li>
<li>The neural network takes the state as input and outputs Q-values for all possible actions.</li>
<li>The architecture of a DQN typically includes:
<ul>
<li>Input Layer: Receives the state representation (e.g., raw pixels for images).</li>
<li>Hidden Layers: Multiple fully connected layers with activation functions (e.g., ReLU)</li>
<li>Output Layer: Outputs Q-values for each possible action.</li>
</ul>
</li>
<li>The DQN is trained using the Q-learning update rule, but instead of updating a Q-table, we update the weights of the neural network.</li>
<li>The loss function used for training is based on the difference between the predicted Q-values and the target Q-values (TD target).</li>
<li>The target Q-values are computed using the immediate reward and the maximum estimated Q-value for the next state, similar to Q-learning</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="preprocessing-the-input-and-temporal-limits">


<h3 class="header-anchor-wrapper">Preprocessing the Input and Temporal Limits
  <a href="#preprocessing-the-input-and-temporal-limits" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<p><img src="/blogs/tutorials/huggingfaceRL/preprocessing.jpg" alt="Preprocessing"></p>
<ul>
<li>In environments with high-dimensional inputs (e.g., images), preprocessing is essential to reduce complexity and improve learning efficiency.</li>
<li>Common preprocessing steps include:
<ul>
<li>Grayscaling: Convert RGB images to grayscale to reduce the number of input channels.</li>
<li>Resizing: Resize images to a smaller, fixed size (e.g., 84x84 pixels), cropping unnecessary parts to focus on relevant information.</li>
<li>Normalization: Scale pixel values to a range (e.g., [0, 1] or [-1, 1]) to improve training stability.</li>
<li>Frame Stacking: Stack multiple consecutive frames to provide temporal context, allowing the agent to infer motion and dynamics.</li>
</ul>
</li>
<li>Temporal limits are imposed on episodes to prevent them from running indefinitely, ensuring that the agent learns to complete tasks within a reasonable time frame.
<img src="/blogs/tutorials/huggingfaceRL/temporal-limitation-2.jpg" alt="Temporal Limits"></li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="the-deep-q-learning-algorithm">


<h3 class="header-anchor-wrapper">The Deep Q-Learning Algorithm
  <a href="#the-deep-q-learning-algorithm" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h3>

<ul>
<li>
<p>We create a loss function that compares our current Q-value estimates with the Q-target and uses gradient descent to update the Deep Q-Network&rsquo;s weights to approximate the Q-values better.
<img src="/blogs/tutorials/huggingfaceRL/Q-target.jpg" alt="Q-target"></p>
</li>
<li>
<p>The Deep Q-Learning algorithm has two phases: the interaction phase and the training phase.</p>
</li>
<li>
<p><strong>Interaction or Sampling Phase</strong>:</p>
<ul>
<li>The agent performs actions and stores the observed experience tuples in a replay memory.</li>
<li>Experience tuples typically include the current state, action taken, reward received, and next state.</li>
<li>This phase allows the agent to explore the environment and gather diverse experiences for training.</li>
</ul>
</li>
<li>
<p><strong>Training Phase</strong>:</p>
<ul>
<li>The agent samples mini-batches of experience tuples from the replay memory to train the Deep Q-Network.</li>
<li>The loss function is computed based on the difference between the predicted Q-values and the target Q-values (TD target).</li>
<li>The network&rsquo;s weights are updated using gradient descent to minimize the loss.</li>
<li>This phase allows the agent to learn from past experiences and improve its policy over time.</li>
</ul>
</li>
</ul>
<p><img src="/blogs/tutorials/huggingfaceRL/sampling-training.jpg" alt="DQN Algorithm"></p>
<ul>
<li>DQN can suffer from instability because of combining a non-linear function approximator (neural network) and bootstrapping (using current estimates to update themselves).</li>
<li>To address this, DQN uses 3 key techniques:
<ol>
<li>Experience Replay to make more efficient use of experiences.</li>
<li>Fixed Q-Target to stabilize the training.</li>
<li>Double Deep Q-Learning, to handle the problem of the overestimation of Q-values.</li>
</ol>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="experience-replay">


<h4 class="header-anchor-wrapper">Experience Replay
  <a href="#experience-replay" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ul>
<li>Experience Replay stores the agent&rsquo;s experiences in a replay memory (buffer) and samples mini-batches from this memory to train the network.</li>
<li>This has two functions:
<ol>
<li>Make more efficient use of the experiences during the training. Usually, in online reinforcement learning, the agent interacts with the environment, gets experiences (state, action, reward, and next state), learns from them (updates the neural network), and discards them. This is not efficient. By storing experiences in a replay memory, we can reuse them multiple times for training, <strong>improving sample efficiency</strong>.</li>
<li>Avoid forgetting previous experiences (aka catastrophic interference, or catastrophic forgetting) and reduce the correlation between experiences. Catastrophic forgetting happens when the agent learns new information and forgets previously learned information. By sampling randomly from a replay memory, we ensure that the training data is more diverse and less correlated, which helps stabilize learning.</li>
</ol>
</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="fixed-q-target">


<h4 class="header-anchor-wrapper">Fixed Q-Target
  <a href="#fixed-q-target" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ul>
<li>We do not know the real TD target because we do not know the optimal Q-values for the next state.</li>
<li>Bellman equation tells us that the optimal Q-value for the next state is the maximum Q-value over all possible actions in that state.</li>
<li>Problem: We are using the same parameters (weights) of the neural network to estimate both the current Q-values and the target Q-values. So, there&rsquo;s significant correlation between the two, which can lead to instability and divergence during training.</li>
<li>Solution: Use a separate neural network, called the target network, to compute the target Q-values.
<ul>
<li>The target network has the same architecture as the main network but with different weights.</li>
<li>The weights of the target network are updated less frequently (e.g., every few thousand steps) by copying the weights from the main network.</li>
<li>This decouples the target Q-value estimation from the current Q-value estimation, reducing correlation and improving stability.</li>
</ul>
</li>
<li>The target Q-value is computed using the target network:
$$\text{TD Target} = R_{t+1} + \gamma \max_a Q_{\text{target}}(S_{t+1}, a)$$</li>
<li>Where $Q_{\text{target}}$ is the Q-value estimated by the target network.</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="double-deep-q-learning">


<h4 class="header-anchor-wrapper">Double Deep Q-Learning
  <a href="#double-deep-q-learning" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<ul>
<li>This handles the problem of the overestimation of Q-values.</li>
<li>When calculating the target Q-value, we use the main network to select the action that maximizes the Q-value for the next state, but we use the target network to evaluate this action. How are we sure that the best action for the next state is the one that maximizes the Q-value according to the main network? It might be an overestimation.</li>
<li>The accuracy of the Q-values depends on what action we have tried and what states we have explored.</li>
<li>If non-optimal actions are regularly given a higher Q value than the optimal best action, the learning will be complicated.</li>
<li>The solution is to use the Double Q-learning approach:
<ul>
<li>Use the DQN network to select the best action that maximizes the Q-value for the next state.</li>
<li>Use the target network to evaluate this action and compute the target Q-value.</li>
</ul>
</li>
<li>This reduces the overestimation bias because the action selection and evaluation are decoupled.</li>
<li>The target Q-value in Double DQN is computed as:
$$\text{TD Target} = R_{t+1} + \gamma Q_{\text{target}}(S_{t+1}, \arg\max_a Q_{\text{main}}(S_{t+1}, a))$$</li>
<li>Where $Q_{\text{main}}$ is the Q-value estimated by the main network, and $Q_{\text{target}}$ is the Q-value estimated by the target network.</li>
</ul>
</section>
    
        
        
            
            
            
        
        <section id="summary-of-dqn-improvements">


<h4 class="header-anchor-wrapper">Summary of DQN Improvements
  <a href="#summary-of-dqn-improvements" class="header-anchor-link">
<svg
    xmlns="http://www.w3.org/2000/svg"
    width="1rem" height="1rem" viewBox="0 0 24 24" fill="none"
    stroke="currentColor" stroke-width="2" stroke-linecap="round"
    stroke-linejoin="round">
    <line x1="4" y1="9" x2="20" y2="9"></line><line x1="4" y1="15" x2="20" y2="15"></line><line x1="10" y1="3" x2="8" y2="21"></line><line x1="16" y1="3" x2="14" y2="21"></line>
</svg>

</a>
</h4>

<table class="mc-table">
  <thead>
      <tr>
          <th>Technique</th>
          <th>Purpose</th>
          <th>Benefit</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Experience Replay</td>
          <td>Store and reuse experiences</td>
          <td>Improves sample efficiency and reduces correlation between experiences</td>
      </tr>
      <tr>
          <td>Fixed Q-Target</td>
          <td>Use a separate target network for Q-value estimation</td>
          <td>Stabilizes training by reducing correlation between current and target Q-values</td>
      </tr>
      <tr>
          <td>Double DQN</td>
          <td>Decouple action selection and evaluation</td>
          <td>Reduces overestimation bias in Q-value estimates</td>
      </tr>
  </tbody>
</table>
</section>
    
</article>
</div>


                
                    
                

                
            </div>
        </main>
</body>
</html>

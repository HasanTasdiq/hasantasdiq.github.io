<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bits, Bytes &amp; Life on S. M. Hasan</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Bits, Bytes &amp; Life on S. M. Hasan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention-Free Transformer: Escaping the Quadratic Bottleneck</title>
      <link>http://localhost:1313/posts/aft-1/</link>
      <pubDate>Sun, 28 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/aft-1/</guid>
      <description>&lt;!-- end-chunk --&gt;
&lt;!-- begin-chunk data-anchor=&#34;attention-free-transformer-escaping-the-quadratic-bottleneck&#34; --&gt;

&lt;h1 class=&#34;header-anchor-wrapper&#34;&gt;Attention-Free Transformer: Escaping the Quadratic Bottleneck
  &lt;a href=&#34;#attention-free-transformer-escaping-the-quadratic-bottleneck&#34; class=&#34;header-anchor-link&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;1rem&#34; height=&#34;1rem&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;How AFT rethinks attention to achieve linear complexity while preserving performance&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Transformer architecture revolutionized AI, but its self-attention mechanism comes with a fundamental limitation: quadratic complexity $\mathcal{O}(n^2)$ with sequence length. This makes processing long sequences computationally prohibitive. The Attention-Free Transformer (AFT) emerges as an elegant solution that maintains strong performance while achieving linear complexity.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Demystifying Transformers: Attention, Multi-Head Magic, and the Math Behind the Revolution</title>
      <link>http://localhost:1313/posts/transformers-1/</link>
      <pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/transformers-1/</guid>
      <description>&lt;!-- end-chunk --&gt;
&lt;!-- begin-chunk data-anchor=&#34;demystifying-transformers-attention-multi-head-magic-and-the-math-behind-the-revolution&#34; --&gt;

&lt;h1 class=&#34;header-anchor-wrapper&#34;&gt;Demystifying Transformers: Attention, Multi-Head Magic, and the Math Behind the Revolution
  &lt;a href=&#34;#demystifying-transformers-attention-multi-head-magic-and-the-math-behind-the-revolution&#34; class=&#34;header-anchor-link&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;1rem&#34; height=&#34;1rem&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;From single head to multi-head attention - understanding the architectural breakthrough that changed AI forever&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Transformer architecture, introduced in the seminal &amp;ldquo;Attention Is All You Need&amp;rdquo; paper, revolutionized natural language processing by replacing recurrent networks with a purely attention-based approach. At its heart lies the self-attention mechanism - a powerful way for models to understand relationships between all words in a sequence simultaneously.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Demystifying RNNs: A Deep Dive into Dimensions and Parameters</title>
      <link>http://localhost:1313/posts/rnn-1/</link>
      <pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/rnn-1/</guid>
      <description>&lt;!-- end-chunk --&gt;
&lt;!-- begin-chunk data-anchor=&#34;demystifying-rnns-a-deep-dive-into-dimensions-and-parameters&#34; --&gt;

&lt;h1 class=&#34;header-anchor-wrapper&#34;&gt;Demystifying RNNs: A Deep Dive into Dimensions and Parameters
  &lt;a href=&#34;#demystifying-rnns-a-deep-dive-into-dimensions-and-parameters&#34; class=&#34;header-anchor-link&#34;&gt;
&lt;svg
    xmlns=&#34;http://www.w3.org/2000/svg&#34;
    width=&#34;1rem&#34; height=&#34;1rem&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34;
    stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34;
    stroke-linejoin=&#34;round&#34;&gt;
    &lt;line x1=&#34;4&#34; y1=&#34;9&#34; x2=&#34;20&#34; y2=&#34;9&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;4&#34; y1=&#34;15&#34; x2=&#34;20&#34; y2=&#34;15&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;10&#34; y1=&#34;3&#34; x2=&#34;8&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;&lt;line x1=&#34;16&#34; y1=&#34;3&#34; x2=&#34;14&#34; y2=&#34;21&#34;&gt;&lt;/line&gt;
&lt;/svg&gt;

&lt;/a&gt;
&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Understanding what really happens inside Recurrent Neural Networks&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When learning about Recurrent Neural Networks (RNNs), many tutorials focus on the high-level concept of &amp;ldquo;memory&amp;rdquo; but gloss over the practical details of how they actually work. As someone who struggled with these details, I want to share the insights that finally made RNNs click for me.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>